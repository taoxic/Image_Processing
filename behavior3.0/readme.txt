采用了决策树分类，两类分类为65%，六类分类为30%识别率
    调用了matlab自带的机器学习中的函数，但是没有对生成的决策树进行剪枝，存在过拟合的情况，
导致结果不太好，也有可能是图像的特征向量数值不够精确细化

决策树分类运行时间：
	对于二类分类：0.2010s，0.2125s，0.1988s，0.1993
	对于六类分类：0.8130s，0.8112s，0.7716s，0.6830s

CART算法：
	Classification And Regression Tree，即分类回归树算法，简称CART算法，它是决策树的一种实现，通
常决策树主要有三种实现，分别是ID3算法，CART算法和C4.5算法。
   CART算法是一种二分递归分割技术，把当前样本划分为两个子样本，使得生成的每个非叶子结点都有两个分支，
因此CART算法生成的决策树是结构简洁的二叉树。由于CART算法构成的是一个二叉树，它在每一步的决策时只能
是“是”或者“否”，即使一个feature有多个取值，也是把数据分为两部分。在CART算法中主要分为两个步骤
   （1）将样本递归划分进行建树过程
   （2）用验证数据进行剪枝

	
	
决策树的构造：
	策树的构造过程不依赖领域知识，它使用属性选择度量来选择将元组最好地划分成不同的类的属性。
所谓决策树的构造就是进行属性选择度量确定各个特征属性之间的拓扑结构。
	构造决策树的关键步骤是分裂属性。所谓分裂属性就是在某个节点处按照某一特征属性的不同划分构造不同的分支，其目标是让各个分裂子集尽可能地“纯”。
尽可能“纯”就是尽量让一个分裂子集中待分类项属于同一类别。分裂属性分为三种不同的情况：
   1、属性是离散值且不要求生成二叉决策树。此时用属性的每一个划分作为一个分支。
   2、属性是离散值且要求生成二叉决策树。此时使用属性划分的一个子集进行测试，按照“属于此子集”和“不属于此子集”分成两个分支。
   3、属性是连续值。此时确定一个值作为分裂点split_point，按照>split_point和<=split_point生成两个分支。
   构造决策树的关键性内容是进行属性选择度量，属性选择度量是一种选择分裂准则，是将给定的类标记的训练集合的数据划分D“最好”地分成
个体类的启发式方法，它决定了拓扑结构及分裂点split_point的选择。属性选择度量算法有很多，一般使用自顶向下递归分治法，并采用不回溯
的贪心策略。


剪枝：
	在实际构造决策树时，通常要进行剪枝，这时为了处理由于数据中的噪声和离群点导致的过分拟合问题。剪枝有两种：
	
	1、先剪枝――在构造过程中，当某个节点满足剪枝条件，则直接停止此分支的构造。
	2、后剪枝――先构造完成完整的决策树，再通过某些条件遍历树进行剪枝。


